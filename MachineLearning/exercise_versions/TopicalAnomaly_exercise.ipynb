{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicalAnomaly",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly detection exercise\n",
        "\n",
        "We would like to investigate putting an 'anomalous event' detection algorithm in the trigger of a particle physics collider experiment. For this purpose, we will use a 'background' or 'Standard Model' dataset, on which we will train our algorithm to learn about 'normal events'. Then we will apply it to a dataset containing 'abnormal' or 'New Physics' events, containing new particle decays, and see if our algorithm can detect them as being 'abnormal'. Finally, as we're triggering in real time ('online'), we will check the timing of our algorithm.\n",
        "\n",
        "## Datasets\n",
        "Datasets were generated in the context of an online reference / competition, and currently consist of A->4 leptons, leptoquarks -> b tau, h0 -> tau tau, and h+ -> tau nu, pick your favourite New Physics case (or try them all!)\n",
        "\n",
        "## Environment\n",
        "The default environment will be **Google Colab**. You can also make use of the Nikhef in-house jupterlab service **Callysto** at https://callysto.nikhef.nl where you can customize your environment following https://wiki.nikhef.nl/ct/Jupyterlab. You can also run your own jupyter instance locally.\n",
        "\n",
        "**TIP**: to speed up learning in Google Colab, enable hardware acceleration with GPUs by going to \\\\\n",
        "`Runtime > Change runtime type > Hardware accelerator > GPU`.\n",
        "\n",
        "*As prepared by, with full credits to,* https://mpp-hep.github.io/ADC2021/ \\\\\n",
        "*(Edited by jdevries)*\n"
      ],
      "metadata": {
        "id": "qfE7gjeCfAh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = \"COLAB\"\n",
        "#env = \"CALLYSTO\""
      ],
      "metadata": {
        "id": "T65gsYoSDNsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(env == \"CALLYSTO\") :\n",
        "  !pip install h5py tensorflow sklearn"
      ],
      "metadata": {
        "id": "nLvHstp8DR0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nendY3RScwdh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense,ZeroPadding2D, BatchNormalization, Activation, Layer, ReLU, LeakyReLU,Conv2D,AveragePooling2D,UpSampling2D,Reshape,Flatten\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import argparse\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download datasets. These are cleared when deleting the runtime.\n",
        "#  If connection is too slow: they are also hosted on stoomboot at /data/bfys/jdevries/public\n",
        "\n",
        "# Background (Standard Model) dataset\n",
        "bkg_file = \"background_for_training.h5\"\n",
        "if not os.path.exists(bkg_file) :\n",
        "  !wget https://zenodo.org/record/5046389/files/background_for_training.h5\n",
        "\n",
        "# Anomaly dataset: choose your favourite New Physics candidate from https://mpp-hep.github.io/ADC2021/\n",
        "signal_file = 'leptoquark_LOWMASS_lepFilter_13TeV.h5'\n",
        "if not os.path.exists(signal_file) :\n",
        "  !wget https://zenodo.org/record/5055454/files/leptoquark_LOWMASS_lepFilter_13TeV.h5\n",
        "\n",
        "if(env == \"COLAB\") :\n",
        "  !ls"
      ],
      "metadata": {
        "id": "gqJypQnOgdfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Must be TF version 2\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "2WaIwysoc7jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some data handling functions\n",
        "\n",
        "def load_model(model_name, custom_objects=None):\n",
        "    name = model_name + '.json'\n",
        "    json_file = open(name, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model_json, custom_objects=custom_objects)\n",
        "    model.load_weights(model_name + '.h5')\n",
        "    return model\n",
        "\n",
        "def save_model(model_save_name, model):\n",
        "    with open(model_save_name + '.json', 'w') as json_file:\n",
        "        json_file.write(model.to_json())\n",
        "    model.save_weights(model_save_name + '.h5')\n",
        "\n",
        "\n",
        "def create_datasets_dense(bkg_file, output_bkg_name, signals_files, output_signal_names, events=None, test_size=0.2, val_size=0.2, input_shape=57):\n",
        "    \n",
        "    # read BACKGROUND data\n",
        "    with h5py.File(bkg_file, 'r') as file:\n",
        "        full_data = file['Particles'][:,:,:-1]\n",
        "        np.random.shuffle(full_data)\n",
        "        if events: full_data = full_data[:events,:,:]\n",
        "    \n",
        "    # define training, test and validation datasets\n",
        "    X_train, X_test = train_test_split(full_data, test_size=test_size, shuffle=True)\n",
        "    X_train, X_val = train_test_split(X_train, test_size=val_size)\n",
        "\n",
        "    del full_data\n",
        "    \n",
        "    # flatten the data for model input\n",
        "    X_train = X_train.reshape(X_train.shape[0], input_shape)\n",
        "    X_test = X_test.reshape(X_test.shape[0], input_shape)\n",
        "    X_val = X_val.reshape(X_val.shape[0], input_shape)\n",
        "    \n",
        "    with h5py.File(output_bkg_name + '_dataset.h5', 'w') as h5f:\n",
        "        h5f.create_dataset('X_train', data = X_train)\n",
        "        h5f.create_dataset('X_test', data = X_test)\n",
        "        h5f.create_dataset('X_val', data = X_val)\n",
        "    \n",
        "    if signals_files:\n",
        "        # read SIGNAL data\n",
        "        for i, signal_file in enumerate(signals_files):\n",
        "            f = h5py.File(signal_file,'r')\n",
        "            signal_data = f['Particles'][:,:,:-1]\n",
        "            signal_data = signal_data.reshape(signal_data.shape[0],input_shape)\n",
        "            with h5py.File(output_signal_names[i] + '_dataset.h5', 'w') as h5f2:\n",
        "                h5f2.create_dataset('Data', data = signal_data)        \n",
        "    return"
      ],
      "metadata": {
        "id": "hqK24NUbdCrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create numpy arrays from datasets for easy handling\n",
        "\n",
        "output_bkg_name = 'SM'\n",
        "output_sig_name = 'LQ'\n",
        "filename_bkg = output_bkg_name + '_dataset.h5'\n",
        "filename_sig = output_sig_name + '_dataset.h5'\n",
        "events = 1000000\n",
        "\n",
        "if not os.path.exists(filename_sig) :\n",
        "  create_datasets_dense(bkg_file, output_bkg_name, [signal_file], [output_sig_name], events)"
      ],
      "metadata": {
        "id": "K2QLepa3dcqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load background training data\n",
        "with h5py.File(filename_bkg, 'r') as file:\n",
        "    X_train = np.array(file['X_train'])\n",
        "    X_test  = np.array(file['X_test'])\n",
        "    X_val   = np.array(file['X_val'])\n",
        "\n",
        "print(X_train.shape)\n",
        "X_train = np.reshape(X_train, (-1, 19,3,1))\n",
        "X_test  = np.reshape(X_test,  (-1, 19,3,1))\n",
        "X_val   = np.reshape(X_val,   (-1, 19,3,1))\n",
        "\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "SUpOBsmCey_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now loaded the Standard Model (SM) dataset as train, test and validation random splits. \n",
        "\n",
        "The data contains the **19** most interesting 'particles' per collision event:\n",
        "- 'missing transverse energy' object (`MET`)\n",
        "- 4 electrons\n",
        "- 4 muons\n",
        "- 10 jets\n",
        "\n",
        "which are ordered by `p_T`, from high to low, and might be zero-padded in case there are fewer particles of that type in the event.\n",
        "\n",
        "For each 'particle' there are **3** features in the dataset: \n",
        "- transverse momentum (`p_T`)\n",
        "- pseudorapidity 'angle' (`eta`)\n",
        "- azimuthal angle (`phi`)\n"
      ],
      "metadata": {
        "id": "wnHpLOPumiJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For example, let's draw the pT of the highest-pT muon in all training events\n",
        "\n",
        "plt.hist( [ event[5, 0][0] for event in X_train ], bins=100, range=[1, 80] )\n",
        "plt.xlabel(\"p_T [GeV/c]\")"
      ],
      "metadata": {
        "id": "3eCS8GOxpqyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define NN architecture\n",
        "\n",
        "For anomaly detection, we want our algorithm to **only** see the SM dataset to learn about normal, 'non-anomalous' behavior. After training/testing/validation, we then want to move on to the new physics dataset (e.g. leptoquarks) - and see how it performs in labelling that input as 'anomalous'."
      ],
      "metadata": {
        "id": "Mw2T50f4e63N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = (19,3,1)"
      ],
      "metadata": {
        "id": "G_KRfIievEuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### HERE GOES YOUR WORK ###\n",
        "\n",
        "# Write your preferred anomaly detection algorithm here\n",
        "#  that takes as input something of shape=(image_shape)\n",
        "#  and as output something that is relevant for your algorithm, e.g. \n",
        "#  - a value in the N-dimensional hyperspace of your clustering, \n",
        "#  - the 'latent space' of your autoencoder (and the original image after the decoder)\n",
        "#  - the energy / KL-distance of a restricted Boltzmann machine\n",
        "\n",
        "# See e.g.\n",
        "# sklearn.cluster.DBSCAN \n",
        "# sklearn.mixture.BayesianGaussianMixture \n",
        "# sklearn.neural_network.BernoulliRBM\n",
        "# sklearn.svm.OneClassSVM \n",
        "\n",
        "\n",
        "# Trivial autoencoder example (to get you going):\n",
        "latent_dimension = 8\n",
        "\n",
        "#encoder\n",
        "input_encoder = Input(shape=(image_shape))\n",
        "x = Flatten()(input_encoder)\n",
        "x = Dense(latent_dimension)(x)\n",
        "enc = Activation('relu')(x)\n",
        "encoder = Model(inputs=input_encoder, outputs=enc)\n",
        "\n",
        "#decoder\n",
        "x = Dense(19*3)(enc)\n",
        "x = Activation('relu')(x)\n",
        "dec = Reshape(image_shape)(x)\n",
        "\n",
        "autoencoder = Model(inputs=input_encoder, outputs=dec)\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "jJu9Q1L9e7eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer = keras.optimizers.Adam(), loss='mse')"
      ],
      "metadata": {
        "id": "DaLm6dHttYEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 1024"
      ],
      "metadata": {
        "id": "ipdmjbKlHpF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = autoencoder.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
        "                  validation_data=(X_val, X_val))"
      ],
      "metadata": {
        "id": "GZEqAf1UHrb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model_name = 'myAnomalyModel'\n",
        "save_model(model_name, autoencoder)\n",
        "\n",
        "if(env == \"COLAB\") :\n",
        "  !ls"
      ],
      "metadata": {
        "id": "u2L2isk4HxzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Test model on the remainder of our Standard Model dataset, this is our 'reference performance'.\n",
        "print(\"Running predictions for {0} bkg events\".format(len(X_test)))\n",
        "bkg_prediction = autoencoder.predict(X_test)"
      ],
      "metadata": {
        "id": "y5cWTHcvH9ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running on the anomalous data\n",
        "\n",
        "Here we run the 'new physics' data through our trained algorithm, and save the relevant output."
      ],
      "metadata": {
        "id": "tYHSNIPkIq5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the anomalous signal datasets\n",
        "with h5py.File(filename_sig) as file :\n",
        "  signal_data = np.array(file['Data'])\n",
        "\n",
        "signal_data = np.reshape(signal_data, (-1, 19,3,1))"
      ],
      "metadata": {
        "id": "luC9KFijJuyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Run model predictions on anomalous signal\n",
        "print(\"Running predictions for {0} sig events\".format(len(signal_data)))\n",
        "sig_prediction = autoencoder.predict(signal_data)"
      ],
      "metadata": {
        "id": "LKRYbNT3Is4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "save_file = 'myAnomalyModel_results'\n",
        "\n",
        "with h5py.File(save_file, 'w') as file:\n",
        "    file.create_dataset('BKG_input', data=X_test)\n",
        "    file.create_dataset('BKG_predicted', data = bkg_prediction)\n",
        "    file.create_dataset('SIG_input', data=signal_data)\n",
        "    file.create_dataset('SIG_predicted' , data=sig_prediction)"
      ],
      "metadata": {
        "id": "xsK17pA8Mz-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of performance\n",
        "\n",
        "Here is where you assess the metric of performance of your algorithm. \n",
        "\n",
        "For an autoencoder, the 'score' would be *How well can I reconstruct the original image back?*. This would be 'pretty good' if the image is similar to the training data, and 'not so good' if the data looks like something new. As this performance is characterised by the loss function, this is what we will be looking at.\n",
        "\n",
        "For other methods this can be different. For instance, for a Restricted Boltzmann machine it could be the 'Energy', for kernel methods / clustering methods / one-class support vector machines, some 'distance' in some output space, and so on. For your algorithm, consider what it should be doing, and evaluate performance accordingly."
      ],
      "metadata": {
        "id": "ZBcbC62XO_2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(true, prediction):\n",
        "    loss = tf.reduce_mean(tf.math.square(true - prediction),axis=-1)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "SKYuFCIcPCbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute loss value (true, predicted)\n",
        "data_labels = [\"SM\", \"LQ\"]\n",
        "\n",
        "total_loss = []\n",
        "total_loss.append(mse_loss(X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2])),\\\n",
        "                           (bkg_prediction.reshape((bkg_prediction.shape[0],bkg_prediction.shape[1]*bkg_prediction.shape[2]))).astype(np.float32)).numpy())\n",
        "total_loss.append(mse_loss(signal_data.reshape((signal_data.shape[0],signal_data.shape[1]*signal_data.shape[2])),\\\n",
        "                           (sig_prediction.reshape((sig_prediction.shape[0],sig_prediction.shape[1]*sig_prediction.shape[2]))).astype(np.float32)).numpy())"
      ],
      "metadata": {
        "id": "ilacjl60PM5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_size=100\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "for i, label in enumerate(data_labels):\n",
        "    plt.hist(total_loss[i], bins=bin_size, label=label, density = True, histtype='step', fill=False, linewidth=1.5)\n",
        "plt.yscale('log')\n",
        "plt.xlabel(\"Autoencoder Loss\")\n",
        "plt.ylabel(\"Probability (a.u.)\")\n",
        "plt.title('MSE loss')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qiPNE5YSQ2kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When is something anomalous?\n",
        "\n",
        "In order to define something as being 'anomalous', we have to decide on a 'cut point' in the metric defined above. In the case of an autoencoder, we could place a cut on a certain value of the loss: anything higher than, say, 1500, could be considered anomalous. \n",
        "\n",
        "In order to assess the performance regardless of the cut point, we can make a ROC curve - this evaluates the 'true' anomaly rate as function of the 'false' anomaly rate.\n",
        "\n",
        "The area under this curve ('AUC') is a generic metric for comparing performances between models."
      ],
      "metadata": {
        "id": "dW9jZE0FSRSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc"
      ],
      "metadata": {
        "id": "sQXUAw9RRLvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.concatenate([['Standard Model'], ['Leptoquark']])"
      ],
      "metadata": {
        "id": "5Q3FP6ACStui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_background = np.zeros(total_loss[0].shape[0])\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "for i, label in enumerate(labels):    \n",
        "    trueVal = np.concatenate((np.ones(total_loss[i].shape[0]), target_background)) # anomaly=1, bkg=0\n",
        "    predVal_loss = np.concatenate((total_loss[i], total_loss[0]))\n",
        "    fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
        "    auc_loss = auc(fpr_loss, tpr_loss)\n",
        "    \n",
        "    plt.plot(fpr_loss, tpr_loss, \"-\", label='%s (auc = %.1f%%)'%(label,auc_loss*100.), linewidth=1.5)\n",
        "    #plt.semilogx()\n",
        "    #plt.semilogy()\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.legend(loc='center right')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
        "plt.title(\"ROC AE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FYYlKKLcS0QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to improve\n",
        "\n",
        "Now, it's up to you to improve. Can you\n",
        "- Improve upon the model, to enhance the AUC score? How fast does the training converge, and does it help to train multiple EPOCHS? Can you change the model to a different approach, such as a restricted Boltzmann machine or a one-class support vector machine? (And change the 'when is something anomalous' metric accordingly)\n",
        "- Improve the timing of the inference/prediction so it is viable to run in the trigger, without causing too much AUC loss? Can you make a plot of the AUC vs the time of various approaches?\n",
        "- Add the other anomalous datasets in this challenge, and compare how well you can identify e.g. anomalous charged higgs events vs leptoquarks? Maybe some types of new physics signatures are easier to detect than others?"
      ],
      "metadata": {
        "id": "G2ljCXgcaAPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sjk6LNXzmT2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}