{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KH_fIMnXB_aI",
    "outputId": "7ad80d4c-0010-48c8-c1a2-181dbbd4e011"
   },
   "outputs": [],
   "source": [
    "# training GANS is a slow process: be sure to switch to hardware accelerator GPU in 'change runtime type', and switch to high-RAM when prompted.\n",
    "# GAN procedure mostly taken from https://github.com/yandexdataschool/mlhep2018/ \n",
    "\n",
    "# Download dataset\n",
    "!wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt\n",
    "!wget http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz # 233 MB!\n",
    "!tar -xf lfw-funneled.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnRedA7781oU"
   },
   "outputs": [],
   "source": [
    "# Some data-handling tools and imports\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from imageio import imread\n",
    "#from scipy.misc import imresize\n",
    "#!pip install pillow\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_lfw_dataset(attrs_name = \"lfw_attributes.txt\",\n",
    "                      images_name = \"lfw-deepfunneled\",\n",
    "                      raw_images_name = \"lfw\",\n",
    "                      use_raw=False,\n",
    "                      dx=80,dy=80,\n",
    "                      dimx=45,dimy=45):\n",
    "\n",
    "    # read attrs\n",
    "    # the header row begins with a #, which we want to ignore\n",
    "    with open(attrs_name) as attributes_file:\n",
    "        attributes_file.readline()\n",
    "        ugly_header = attributes_file.read(2)\n",
    "        assert ugly_header == \"#\\t\"\n",
    "        df_attrs = pd.read_csv(attributes_file, sep='\\t', skipinitialspace=True)\n",
    "        #print(df_attrs) \n",
    "        #print(df_attrs.columns.values.tolist())\n",
    "    \n",
    "    #read photos\n",
    "    dirname = raw_images_name if use_raw else images_name\n",
    "    photo_ids = []\n",
    "    initial_depth = dirname.count(os.sep)\n",
    "    for dirpath, dirnames, filenames in os.walk(dirname):\n",
    "        if dirpath.count(os.sep) - initial_depth > 1:\n",
    "            continue\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".jpg\"):\n",
    "                photo_id = fname[:-4].replace('_',' ').split()\n",
    "                person_id = ' '.join(photo_id[:-1])\n",
    "                photo_number = int(photo_id[-1])\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "\n",
    "    # mass-merge\n",
    "    # (photos now have same order as attributes)\n",
    "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
    "\n",
    "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
    "\n",
    "    #image preprocessing\n",
    "    all_photos =df['photo_path'].apply(imread)\\\n",
    "                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
    "                                .apply(lambda img: Image.fromarray(img).resize([dimx,dimy]) )\n",
    "\n",
    "    all_photos = np.stack(all_photos.values).astype('uint8')\n",
    "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"], axis=1)\n",
    "    \n",
    "    return all_photos, all_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyFG9F6b_P1R"
   },
   "outputs": [],
   "source": [
    "# Fetch the datasets of faces\n",
    "\n",
    "#%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "plt.rcParams.update({'axes.titlesize': 'small'})\n",
    "\n",
    "# a utility to load the dataset\n",
    "data, _ = fetch_lfw_dataset(dimx=36, dimy=36,\n",
    "                            images_name=\"lfw_funneled\",\n",
    "                            attrs_name=\"lfw_attributes.txt\",\n",
    "                            )\n",
    "\n",
    "# preprocess faces\n",
    "data = np.float32(data)/255.\n",
    "\n",
    "IMG_SHAPE = data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "T1-_inEph8dO",
    "outputId": "fb7dd1a9-c3be-4ad4-e3b0-e64747fbe66b"
   },
   "outputs": [],
   "source": [
    "# show some random (real) faces from our dataset\n",
    "\n",
    "plt.figure(figsize=[18, 18])\n",
    "plt.axis('off');\n",
    "\n",
    "for i in range(5):\n",
    "  plt.subplot(1,5,i+1)\n",
    "  plt.imshow(data[np.random.randint(data.shape[0])], cmap=\"gray\", interpolation=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuCdxa1wGT2K"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Please don't run tensorfow without this config. Without it you'll take the whole memory of the GPU\n",
    "# and make it unusable by anyone else\n",
    "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "#sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# Edit: for colab, resources are limited on the server-side. Go hog.\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers as L\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJEchmd_VM9b"
   },
   "outputs": [],
   "source": [
    "# If we can't tell good faces from bad, we delegate it to yet another neural network!\n",
    "# That makes two of them:\n",
    "# Generator - takes random noise for inspiration and tries to generate a face sample.\n",
    "# Let's call him G(z), where z is a gaussian noize.\n",
    "# Discriminator - takes a face sample and tries to tell if it's real or fake.\n",
    "# Predicts the probability of input image being a real face\n",
    "# Let's call him D(x), x being an image.\n",
    "# D(x) is a prediction for real image and D(G(z)) is prediction for the face made by generator.\n",
    "\n",
    "# Now it's your choise which GAN to build: Jensen-Shannon, or Wasserstein \n",
    "# In addition to the theoretical difference, a couple of practical matters: \n",
    "#  Jensen-Shannon GAN should learn several times faster, but is more sensitive to mode collapse and vanishing gradients. \n",
    "#  Wasserstein GAN doesn't go well in company of batch normalization and ELU activation.\n",
    "\n",
    "GAN_TYPE = \"Jensen-Shannon\"\n",
    "#GAN_TYPE = \"Wasserstein\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lcqgo4RiVNwv",
    "outputId": "43e05e46-354a-4e45-8dc8-21a66d590a10"
   },
   "outputs": [],
   "source": [
    "# Make the Generator. \n",
    "#  It takes a random sample as input (size CODE_SIZE) and generates a face with FIGURE_SIZE output size.\n",
    "#  To enlarge from CODE_SIZE to FIGURE_SIZE (#pix_x by #pix_y by #rgb) you need to use \n",
    "#  some Deconvolution2D and UpSampling2D layers.\n",
    "\n",
    "CODE_SIZE = 256\n",
    "\n",
    "# Activations experimentally selected. Will most likely work\n",
    "# for other combinations of activations/architectures\n",
    "if GAN_TYPE == \"Wasserstein\":\n",
    "    generator_activation = partial(keras.activations.relu, alpha=0)\n",
    "elif GAN_TYPE == \"Jensen-Shannon\":\n",
    "    generator_activation = keras.activations.elu\n",
    "\n",
    "with tf.name_scope(\"Generator\"):\n",
    "    # A few Deconv layers. Feel free to add more, but that will make learning even slower\n",
    "    generator = Sequential(name=\"Generator\")\n",
    "    generator.add(L.InputLayer([CODE_SIZE],name='noise'))\n",
    "    generator.add(L.Dense(10*8*8, activation=generator_activation))\n",
    "\n",
    "    ### STUDENT CODE HERE ###\n",
    "    ### --> Write the network for the Generator.\n",
    "    ###  It takes a random sample as input (size CODE_SIZE) and generates a face with FIGURE_SIZE output size.\n",
    "    ###  To enlarge from CODE_SIZE to FIGURE_SIZE (#pix_x by #pix_y by #rgb) you need to use \n",
    "    ###  some Conv2DTranspose and UpSampling2D layers. \n",
    "    ###  Make sure to use the 'generator_activation' as activation.\n",
    "    ###  A typical network size has ~300,000 free parameters.\n",
    "    ### END STUDENT CODE ###\n",
    "    \n",
    "    generator.add(L.Conv2D(3,kernel_size=3,activation=\"tanh\"))\n",
    "\n",
    "    assert generator.output_shape[1:] == IMG_SHAPE, \\\n",
    "    \"generator must output an image of shape %s, but instead it produces %s\" % \\\n",
    "        (IMG_SHAPE, generator.output_shape[1:])\n",
    "\n",
    "    generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwfLGErCVXoL",
    "outputId": "ca834d0f-5386-4c87-ef96-650830a52b9c"
   },
   "outputs": [],
   "source": [
    "# Make the Discriminator.\n",
    "\n",
    "# It is your usual convolutional network with interlooping convolution and pooling layers:\n",
    "#  It takes a figure as input, and a simple output to separate \"Yes\" (figure is real) and \"No\" (figure is fake)\n",
    "# The network does not include dropout/batchnorm to avoid learning complications.\n",
    "# We also regularize the pre-output layer to prevent discriminator from being too certain.\n",
    "\n",
    "discriminator_activation = partial(keras.activations.relu, alpha=0.3)\n",
    "\n",
    "with tf.name_scope(\"Discriminator\"):\n",
    "    discriminator = Sequential(name=\"Discriminator\")\n",
    "    discriminator.add(L.InputLayer(IMG_SHAPE))\n",
    "\n",
    "    ### STUDENT CODE HERE ###\n",
    "    ### Build the network for the Discriminator. \n",
    "    ### Typically described as 'Roughly the inverse of the generator'.\n",
    "    ### As the discriminator needs to learn complex features, a series of multiple Conv2D \n",
    "    ###  layers is recommended, alternated with some MaxPool2D layers to filter out the relevant kernels.\n",
    "    ### A final Flatten and Dense layer is recommended to learn the interesting combinations of kernels.\n",
    "    ### Be sure to use the discriminator_activation as activation for each layer.\n",
    "    ### A typical network has a size of ~400,000 free parameters.\n",
    "    ### END STUDENT CODE ###\n",
    "    \n",
    "    if GAN_TYPE == \"Wasserstein\":\n",
    "        # Wasserstein discriminator values are unconstrained\n",
    "        discriminator.add(L.Dense(1))\n",
    "    elif GAN_TYPE == \"Jensen-Shannon\":\n",
    "        # Jensen-Shannon expects probabilities\n",
    "        discriminator.add(L.Dense(2, activation=tf.nn.log_softmax))\n",
    "\n",
    "    discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O-mJFIVWDzP"
   },
   "outputs": [],
   "source": [
    "# Helper function to create a shuffling image stream for training\n",
    "\n",
    "def get_tf_dataset(dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Produces an infinite stram of Tensorflow batches from a numpy dataset. The dataset is shuffled every epoch.\n",
    "    Args:\n",
    "       dataset: np.array[n_examples, ...]\n",
    "       batch_size: int, batch size of the results\n",
    "    Reuturns:\n",
    "       Tensor, containing the next batch\n",
    "    \"\"\"\n",
    "    if isinstance(dataset, tf.Tensor):\n",
    "        N_EXAMPLES = dataset.shape[0]\n",
    "    else:\n",
    "        N_EXAMPLES = dataset[0].shape[0]\n",
    "    shuffler = tf.data.experimental.shuffle_and_repeat(N_EXAMPLES)\n",
    "    dataset_tf = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    suffled_ds = shuffler(dataset_tf)\n",
    "    #return suffled_ds.batch(batch_size).prefetch(1).make_one_shot_iterator().get_next()\n",
    "    return tf.compat.v1.data.make_one_shot_iterator( suffled_ds.batch(batch_size) ).get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzBIUJVNWc8t"
   },
   "outputs": [],
   "source": [
    "# Initialize training data setup.\n",
    "\n",
    "# Notes on strategy: we will train the two networks concurrently:\n",
    "#  - Train discriminator to better distinguish real data from current generator\n",
    "#  - Train generator to make discriminator think generator is real\n",
    "# Since discriminator is a differentiable neural network, we train both with gradient descent.\n",
    "# Training is done iteratively until discriminator is no longer able to find the difference (or until you run out of patience).\n",
    "# Tricks:\n",
    "# Train generator with adam to speed up training. Discriminator trains with SGD to avoid problems with momentum.\n",
    "# More: https://github.com/soumith/ganhacks\n",
    "\n",
    "# Obtain the training data faces stream for the discriminator\n",
    "train_batch_size = 100\n",
    "real_data = get_tf_dataset(data, train_batch_size)\n",
    "\n",
    "# Generate the noise data to be used in the generator training\n",
    "noise_batch_size = tf.compat.v1.placeholder(tf.int32, shape=[], name=\"noise_batch_size\")\n",
    "noise = tf.random.normal([noise_batch_size, CODE_SIZE], dtype=tf.float32, name=\"noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wmhAaGiWjoE"
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "\n",
    "  # Run data and noise through the networks\n",
    "  discriminator_real = discriminator(real_data)\n",
    "  generated_data = generator(noise)\n",
    "  discriminator_generated = discriminator(generated_data)\n",
    "\n",
    "  # Configure custom learning and loss details, specific to the GAN strategy type\n",
    "\n",
    "  if GAN_TYPE == \"Wasserstein\":\n",
    "    with tf.name_scope(\"gradient_loss\"):\n",
    "        alpha = tf.random_uniform(shape=[tf.shape(generated_data)[0], 1, 1, 1], minval=0., maxval=1.)\n",
    "        interpolates = alpha*real_data + ((1.-alpha)*generated_data)\n",
    "        disc_interpolates = discriminator(interpolates)\n",
    "        gradients = tf.gradients(disc_interpolates, [interpolates])[0]\n",
    "        slopes = tf.norm(tf.reshape(gradients, [tf.shape(gradients)[0], -1]), axis=1)\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.))\n",
    "    EMD_loss = tf.reduce_mean(discriminator_generated) - tf.reduce_mean(discriminator_real)\n",
    "    LAMBDA = 10.\n",
    "    discriminator_loss = EMD_loss + LAMBDA*gradient_penalty\n",
    "    generator_loss = -tf.reduce_mean(discriminator_generated)\n",
    "\n",
    "  if GAN_TYPE == \"Jensen-Shannon\":\n",
    "    logp_real = discriminator(real_data)\n",
    "    logp_gen = discriminator(generated_data)\n",
    "    discriminator_loss = -tf.reduce_mean(logp_real[:,1] + logp_gen[:,0])\n",
    "    generator_loss = -tf.reduce_mean(logp_gen[:,1])\n",
    "\n",
    "# Get gradients\n",
    "disc_grads = disc_tape.gradient(discriminator_loss, discriminator.trainable_weights)\n",
    "gen_grads  = gen_tape.gradient(generator_loss, generator.trainable_weights)\n",
    "\n",
    "# Define the optimizer for both networks.\n",
    "# The values below are rough suggestions aimed at not exploding a discriminator\n",
    "# of complexity roughly equal to the complexity of the generator.\n",
    "disc_learning_rate = 1e-3\n",
    "\n",
    "if GAN_TYPE == \"Wasserstein\":\n",
    "    # https://arxiv.org/pdf/1704.00028.pdf\n",
    "    #disc_optimizer = tf.keras.optimizers.Adam(disc_learning_rate, beta1=0, beta2=0.9).minimize(\n",
    "    #      discriminator_loss, var_list=discriminator.trainable_weights)\n",
    "    disc_optimizer = tf.keras.optimizers.Adam(disc_learning_rate, beta1=0, beta2=0.9).apply_gradients(\n",
    "        zip(disc_grads, discriminator.trainable_weights))\n",
    "elif GAN_TYPE == \"Jensen-Shannon\":\n",
    "    #disc_optimizer = tf.keras.optimizers.SGD(disc_learning_rate).minimize(\n",
    "    #    discriminator_loss, var_list=discriminator.trainable_weights)\n",
    "    disc_optimizer = tf.keras.optimizers.SGD(disc_learning_rate).apply_gradients(\n",
    "        zip(disc_grads, discriminator.trainable_weights))    \n",
    "\n",
    "\n",
    "if GAN_TYPE == \"Wasserstein\":\n",
    "    # https://arxiv.org/pdf/1704.00028.pdf\n",
    "    #gen_optimizer = tf.keras.optimizers.Adam(1e-4, beta1=0, beta2=0.9).minimize(\n",
    "    #    generator_loss, var_list=generator.trainable_weights)\n",
    "    gen_optimizer = tf.keras.optimizers.Adam(1e-4, beta1=0, beta2=0.9).apply_gradients(\n",
    "        zip(gen_grads, generator.trainable_weights))\n",
    "elif GAN_TYPE == \"Jensen-Shannon\":\n",
    "    #gen_optimizer = tf.keras.optimizers.Adam(1e-4).minimize(generator_loss, var_list=generator.trainable_weights)\n",
    "    gen_optimizer = tf.keras.optimizers.Adam(1e-4).apply_gradients(\n",
    "        zip(gen_grads, generator.trainable_weights))\n",
    "\n",
    "learning_summary = tf.compat.v1.summary.merge([\n",
    "    tf.compat.v1.summary.scalar(\"discriminator_loss\", discriminator_loss),\n",
    "    tf.compat.v1.summary.scalar(\"generator_loss\", generator_loss)\n",
    "])\n",
    "\n",
    "images_summary = tf.compat.v1.summary.image(\"generated_images\", generator(noise))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMhPM_fCZ1ES"
   },
   "outputs": [],
   "source": [
    "# initialize all variables.\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVVcyUnUaGMh"
   },
   "outputs": [],
   "source": [
    "# Define helper functions to evaluate the Gen. and Disc. over some data batches\n",
    "\n",
    "def sample_noise_batch(bsize):\n",
    "    # Get a new batch of noise samples\n",
    "    return np.random.normal(size=(bsize, CODE_SIZE)).astype('float32')\n",
    "\n",
    "def sample_data_batch(bsize):\n",
    "    # Get a batch of real faces\n",
    "    idxs = np.random.choice(np.arange(data.shape[0]), size=bsize)\n",
    "    return data[idxs]\n",
    "\n",
    "def sample_images(nrow,ncol, sharp=False):\n",
    "    # Let the generator create some faces from noise and show them\n",
    "    images = generator.predict(sample_noise_batch(bsize=nrow*ncol))\n",
    "    if np.var(images) != 0:\n",
    "        images = images.clip(np.min(data),np.max(data))\n",
    "    for i in range(nrow*ncol):\n",
    "        plt.subplot(nrow,ncol,i+1)\n",
    "        if sharp:\n",
    "            plt.imshow(images[i].reshape(IMG_SHAPE), cmap=\"gray\", interpolation=\"none\")\n",
    "        else:\n",
    "            plt.imshow(images[i].reshape(IMG_SHAPE), cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "\n",
    "def sample_probas(bsize):\n",
    "    # Let the discriminator predict 'True' or 'False' label for some real and some generated faces.\n",
    "    #  (note that this is actually a continuous number - a sort of 'probability')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Generated vs real data')\n",
    "    ax.hist(discriminator.predict(sample_data_batch(bsize)).ravel(),\n",
    "             label='D(x)', alpha=0.5, density=True)\n",
    "    ax.hist(discriminator.predict(generator.predict(sample_noise_batch(bsize))).ravel(),\n",
    "             label='D(G(z))', alpha=0.5,  density=True)\n",
    "    ax.legend(loc='best')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "cz5LzfqtaKIt",
    "outputId": "d688a195-839c-47e1-99ea-2297d101d270"
   },
   "outputs": [],
   "source": [
    "# Actual training\n",
    "\n",
    "from IPython import display\n",
    "import os\n",
    "\n",
    "LOGDIR = \"./\"\n",
    "MODEL_NAME = \"faces_GAN_%s_noreg_v1\" % GAN_TYPE\n",
    "MODEL_DIR = \"./\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "MODEL_WEIGHTS_FILE =  os.path.join(MODEL_DIR, (\"%s.ckpt\" % MODEL_NAME))\n",
    "VALIDATION_INTERVAL = 50 # time between intermediate visual updates.\n",
    "TOTAL_ITERATIONS = int(5e3) # was 5e4 but colab takes too long \n",
    "# Number of discriminator training iterations per generator iteration\n",
    "# In our tests for discriminator of roughly as complexity as discriminator\n",
    "# 5 worked for both Wasserstein and Jensenâ€“Shannon.\n",
    "DISCRIMINATOR_ITERATIONS = 5 \n",
    "\n",
    "# Save the intermediate weights, such that if our training gets interrupted,\n",
    "#  We don't have to restart from scratch.\n",
    "train_writer = tf.compat.v1.summary.FileWriter(os.path.join(LOGDIR, MODEL_NAME, \"train\"))\n",
    "train_writer.add_graph(tf.compat.v1.get_default_graph())\n",
    "weights_saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "if(os.path.exists(MODEL_WEIGHTS_FILE)) :\n",
    "  try:\n",
    "    weights_saver.restore(sess, MODEL_WEIGHTS_FILE)\n",
    "  except (tf.errors.NotFoundError, tf.errors.InvalidArgumentError):\n",
    "    print(\"Can't restore parameters: no file with weights\")\n",
    "\n",
    "\n",
    "for epoch in range(TOTAL_ITERATIONS):\n",
    "    for i in range(DISCRIMINATOR_ITERATIONS):\n",
    "        # Train the discriminator\n",
    "        sess.run(disc_optimizer, {noise_batch_size: train_batch_size})\n",
    "    # Train the generator\n",
    "    summary, _ = sess.run([learning_summary, gen_optimizer], {noise_batch_size: train_batch_size})\n",
    "\n",
    "    # write the updated weights\n",
    "    train_writer.add_summary(summary, epoch)\n",
    "    \n",
    "    if epoch % VALIDATION_INTERVAL == 0:\n",
    "        # display intermediate status and some generated faces.\n",
    "        display.clear_output(wait=False)\n",
    "        weights_saver.save(sess, MODEL_WEIGHTS_FILE)\n",
    "        epoch_images_summary = sess.run(images_summary, {noise_batch_size: 3})\n",
    "        train_writer.add_summary(epoch_images_summary, epoch)\n",
    "        sample_images(2, 3, True)\n",
    "        fig, ax = sample_probas(1000)\n",
    "        ax.set_title((\"Epoch %i; \" % epoch) + ax.get_title())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H09ukireaN9R",
    "outputId": "5a35fd24-0af0-4468-9bb0-65b62ec4da4b"
   },
   "outputs": [],
   "source": [
    "## Show us some generated faces after our training!\n",
    "plt.figure(figsize=[16, 24])\n",
    "sample_images(16, 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCl0Xfu5asBX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ML_FLW_GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
